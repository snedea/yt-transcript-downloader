services:
  caddy:
    build:
      context: ./caddy
      dockerfile: Dockerfile
    ports:
      - "80:80"
      - "443:443"
    environment:
      - CF_API_TOKEN=${CF_API_TOKEN}
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    restart: always
    depends_on:
      - frontend
      - backend

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    expose:
      - "8000"
    environment:
      - ENVIRONMENT=production
      - CORS_ORIGINS=http://localhost:3000,http://yt.llam.ai,https://yt.llam.ai,http://100.70.254.21
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - transcript_cache:/app/data
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=
    expose:
      - "3000"
    environment:
      - NEXT_PUBLIC_API_URL=
    restart: always
    depends_on:
      backend:
        condition: service_healthy

volumes:
  transcript_cache:
    name: yt-transcript-cache
  caddy_data:
  caddy_config:
